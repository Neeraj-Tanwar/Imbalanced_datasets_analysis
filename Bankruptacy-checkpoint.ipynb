{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91956\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0,1,2,5,6,8,9,10,13,17,21,23,24,28,34,35,37,47,50,54,56,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\91956\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (14,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\91956\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0,1,2,5,6,7,9,10,13,14,15,16,17,21,23,24,25,28,33,34,35,37,47,49,50,56,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df1=pd.read_csv('1year.csv')\n",
    "df2=pd.read_csv('2year.csv')\n",
    "df3=pd.read_csv('3year.csv')\n",
    "df4=pd.read_csv('4year.csv')\n",
    "df5=pd.read_csv('5year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7012, 65)\n",
      "(10173, 65)\n",
      "(10476, 65)\n",
      "(9539, 65)\n",
      "(5427, 65)\n"
     ]
    }
   ],
   "source": [
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "print(df3.shape)\n",
    "print(df4.shape)\n",
    "print(df5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.20055</td>\n",
       "      <td>0.37951</td>\n",
       "      <td>0.39641</td>\n",
       "      <td>2.0472</td>\n",
       "      <td>32.351</td>\n",
       "      <td>0.38825</td>\n",
       "      <td>0.24976</td>\n",
       "      <td>1.3305</td>\n",
       "      <td>1.1389</td>\n",
       "      <td>0.50494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12196</td>\n",
       "      <td>0.39718</td>\n",
       "      <td>0.87804</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>8.416</td>\n",
       "      <td>5.1372</td>\n",
       "      <td>82.658</td>\n",
       "      <td>4.4158</td>\n",
       "      <td>7.4277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.20912</td>\n",
       "      <td>0.49988</td>\n",
       "      <td>0.47225</td>\n",
       "      <td>1.9447</td>\n",
       "      <td>14.786</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25834</td>\n",
       "      <td>0.99601</td>\n",
       "      <td>1.6996</td>\n",
       "      <td>0.49788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>0.42002</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0</td>\n",
       "      <td>4.1486</td>\n",
       "      <td>3.2732</td>\n",
       "      <td>107.35</td>\n",
       "      <td>3.4</td>\n",
       "      <td>60.987</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.24866</td>\n",
       "      <td>0.69592</td>\n",
       "      <td>0.26713</td>\n",
       "      <td>1.5548</td>\n",
       "      <td>-1.1523</td>\n",
       "      <td>0</td>\n",
       "      <td>0.30906</td>\n",
       "      <td>0.43695</td>\n",
       "      <td>1.309</td>\n",
       "      <td>0.30408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.24114</td>\n",
       "      <td>0.81774</td>\n",
       "      <td>0.76599</td>\n",
       "      <td>0.69484</td>\n",
       "      <td>4.9909</td>\n",
       "      <td>3.951</td>\n",
       "      <td>134.27</td>\n",
       "      <td>2.7185</td>\n",
       "      <td>5.2078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.081483</td>\n",
       "      <td>0.30734</td>\n",
       "      <td>0.45879</td>\n",
       "      <td>2.4928</td>\n",
       "      <td>51.952</td>\n",
       "      <td>0.14988</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>1.8661</td>\n",
       "      <td>1.0571</td>\n",
       "      <td>0.57353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054015</td>\n",
       "      <td>0.14207</td>\n",
       "      <td>0.94598</td>\n",
       "      <td>0</td>\n",
       "      <td>4.5746</td>\n",
       "      <td>3.6147</td>\n",
       "      <td>86.435</td>\n",
       "      <td>4.2228</td>\n",
       "      <td>5.5497</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.18732</td>\n",
       "      <td>0.61323</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>1.4063</td>\n",
       "      <td>-7.3128</td>\n",
       "      <td>0.18732</td>\n",
       "      <td>0.18732</td>\n",
       "      <td>0.6307</td>\n",
       "      <td>1.1559</td>\n",
       "      <td>0.38677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13485</td>\n",
       "      <td>0.48431</td>\n",
       "      <td>0.86515</td>\n",
       "      <td>0.12444</td>\n",
       "      <td>6.3985</td>\n",
       "      <td>4.3158</td>\n",
       "      <td>127.21</td>\n",
       "      <td>2.8692</td>\n",
       "      <td>7.898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5422</td>\n",
       "      <td>0.012898</td>\n",
       "      <td>0.70621</td>\n",
       "      <td>0.038857</td>\n",
       "      <td>1.1722</td>\n",
       "      <td>-18.907</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>0.416</td>\n",
       "      <td>1.6768</td>\n",
       "      <td>0.29379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020169</td>\n",
       "      <td>0.043904</td>\n",
       "      <td>1.0122</td>\n",
       "      <td>1.2594</td>\n",
       "      <td>13.472</td>\n",
       "      <td>12.432</td>\n",
       "      <td>49.117</td>\n",
       "      <td>7.4313</td>\n",
       "      <td>2.2799</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5423</td>\n",
       "      <td>-0.57805</td>\n",
       "      <td>0.96702</td>\n",
       "      <td>-0.80085</td>\n",
       "      <td>0.16576</td>\n",
       "      <td>-67.365</td>\n",
       "      <td>-0.57805</td>\n",
       "      <td>-0.57805</td>\n",
       "      <td>-0.40334</td>\n",
       "      <td>0.93979</td>\n",
       "      <td>-0.39004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064073</td>\n",
       "      <td>1.482</td>\n",
       "      <td>1.0641</td>\n",
       "      <td>-0.018084</td>\n",
       "      <td>110.72</td>\n",
       "      <td>44.759</td>\n",
       "      <td>81.22</td>\n",
       "      <td>4.494</td>\n",
       "      <td>5.1305</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5424</td>\n",
       "      <td>-0.17905</td>\n",
       "      <td>1.2553</td>\n",
       "      <td>-0.27599</td>\n",
       "      <td>0.74554</td>\n",
       "      <td>-120.44</td>\n",
       "      <td>-0.17905</td>\n",
       "      <td>-0.15493</td>\n",
       "      <td>-0.26018</td>\n",
       "      <td>1.1749</td>\n",
       "      <td>-0.32659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14888</td>\n",
       "      <td>0.54824</td>\n",
       "      <td>0.85112</td>\n",
       "      <td>-0.52243</td>\n",
       "      <td>9.8526</td>\n",
       "      <td>3.4892</td>\n",
       "      <td>207.87</td>\n",
       "      <td>1.7559</td>\n",
       "      <td>9.9527</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5425</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>0.74394</td>\n",
       "      <td>0.015449</td>\n",
       "      <td>1.0878</td>\n",
       "      <td>-17.003</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.10918</td>\n",
       "      <td>0.12531</td>\n",
       "      <td>0.84516</td>\n",
       "      <td>0.093224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1832</td>\n",
       "      <td>-1.1677</td>\n",
       "      <td>1.1832</td>\n",
       "      <td>6.0924</td>\n",
       "      <td>13.886</td>\n",
       "      <td>6.0769</td>\n",
       "      <td>83.122</td>\n",
       "      <td>4.3911</td>\n",
       "      <td>0.95575</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5426</td>\n",
       "      <td>-0.10537</td>\n",
       "      <td>0.53629</td>\n",
       "      <td>-0.045578</td>\n",
       "      <td>0.91478</td>\n",
       "      <td>-56.068</td>\n",
       "      <td>-0.10537</td>\n",
       "      <td>-0.10994</td>\n",
       "      <td>0.8646</td>\n",
       "      <td>0.9504</td>\n",
       "      <td>0.46367</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052186</td>\n",
       "      <td>-0.22725</td>\n",
       "      <td>1.0522</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>7.7332</td>\n",
       "      <td>4.7174</td>\n",
       "      <td>136.85</td>\n",
       "      <td>2.6672</td>\n",
       "      <td>2.7927</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42627 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Attr1    Attr2      Attr3    Attr4    Attr5     Attr6     Attr7  \\\n",
       "0      0.20055  0.37951    0.39641   2.0472   32.351   0.38825   0.24976   \n",
       "1      0.20912  0.49988    0.47225   1.9447   14.786         0   0.25834   \n",
       "2      0.24866  0.69592    0.26713   1.5548  -1.1523         0   0.30906   \n",
       "3     0.081483  0.30734    0.45879   2.4928   51.952   0.14988  0.092704   \n",
       "4      0.18732  0.61323     0.2296   1.4063  -7.3128   0.18732   0.18732   \n",
       "...        ...      ...        ...      ...      ...       ...       ...   \n",
       "5422  0.012898  0.70621   0.038857   1.1722  -18.907         0  0.013981   \n",
       "5423  -0.57805  0.96702   -0.80085  0.16576  -67.365  -0.57805  -0.57805   \n",
       "5424  -0.17905   1.2553   -0.27599  0.74554  -120.44  -0.17905  -0.15493   \n",
       "5425  -0.10886  0.74394   0.015449   1.0878  -17.003  -0.10886  -0.10918   \n",
       "5426  -0.10537  0.53629  -0.045578  0.91478  -56.068  -0.10537  -0.10994   \n",
       "\n",
       "         Attr8    Attr9    Attr10  ...    Attr56    Attr57   Attr58  \\\n",
       "0       1.3305   1.1389   0.50494  ...   0.12196   0.39718  0.87804   \n",
       "1      0.99601   1.6996   0.49788  ...    0.1213   0.42002    0.853   \n",
       "2      0.43695    1.309   0.30408  ...   0.24114   0.81774  0.76599   \n",
       "3       1.8661   1.0571   0.57353  ...  0.054015   0.14207  0.94598   \n",
       "4       0.6307   1.1559   0.38677  ...   0.13485   0.48431  0.86515   \n",
       "...        ...      ...       ...  ...       ...       ...      ...   \n",
       "5422     0.416   1.6768   0.29379  ...  0.020169  0.043904   1.0122   \n",
       "5423  -0.40334  0.93979  -0.39004  ... -0.064073     1.482   1.0641   \n",
       "5424  -0.26018   1.1749  -0.32659  ...   0.14888   0.54824  0.85112   \n",
       "5425   0.12531  0.84516  0.093224  ...   -0.1832   -1.1677   1.1832   \n",
       "5426    0.8646   0.9504   0.46367  ... -0.052186  -0.22725   1.0522   \n",
       "\n",
       "         Attr59  Attr60  Attr61  Attr62  Attr63   Attr64 class  \n",
       "0      0.001924   8.416  5.1372  82.658  4.4158   7.4277     0  \n",
       "1             0  4.1486  3.2732  107.35     3.4   60.987     0  \n",
       "2       0.69484  4.9909   3.951  134.27  2.7185   5.2078     0  \n",
       "3             0  4.5746  3.6147  86.435  4.2228   5.5497     0  \n",
       "4       0.12444  6.3985  4.3158  127.21  2.8692    7.898     0  \n",
       "...         ...     ...     ...     ...     ...      ...   ...  \n",
       "5422     1.2594  13.472  12.432  49.117  7.4313   2.2799     1  \n",
       "5423  -0.018084  110.72  44.759   81.22   4.494   5.1305     1  \n",
       "5424   -0.52243  9.8526  3.4892  207.87  1.7559   9.9527     1  \n",
       "5425     6.0924  13.886  6.0769  83.122  4.3911  0.95575     1  \n",
       "5426   0.003196  7.7332  4.7174  136.85  2.6672   2.7927     1  \n",
       "\n",
       "[42627 rows x 65 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat([df1,df2,df3,df4,df5],axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace ? with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.replace(['?'],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change complete dataset to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.apply(pd.to_numeric,errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 42627 entries, 0 to 5426\n",
      "Data columns (total 65 columns):\n",
      "Attr1     42627 non-null float64\n",
      "Attr2     42627 non-null float64\n",
      "Attr3     42627 non-null float64\n",
      "Attr4     42627 non-null float64\n",
      "Attr5     42627 non-null float64\n",
      "Attr6     42627 non-null float64\n",
      "Attr7     42627 non-null float64\n",
      "Attr8     42627 non-null float64\n",
      "Attr9     42627 non-null float64\n",
      "Attr10    42627 non-null float64\n",
      "Attr11    42627 non-null float64\n",
      "Attr12    42627 non-null float64\n",
      "Attr13    42627 non-null float64\n",
      "Attr14    42627 non-null float64\n",
      "Attr15    42627 non-null float64\n",
      "Attr16    42627 non-null float64\n",
      "Attr17    42627 non-null float64\n",
      "Attr18    42627 non-null float64\n",
      "Attr19    42627 non-null float64\n",
      "Attr20    42627 non-null float64\n",
      "Attr21    42627 non-null float64\n",
      "Attr22    42627 non-null float64\n",
      "Attr23    42627 non-null float64\n",
      "Attr24    42627 non-null float64\n",
      "Attr25    42627 non-null float64\n",
      "Attr26    42627 non-null float64\n",
      "Attr27    42627 non-null float64\n",
      "Attr28    42627 non-null float64\n",
      "Attr29    42627 non-null float64\n",
      "Attr30    42627 non-null float64\n",
      "Attr31    42627 non-null float64\n",
      "Attr32    42627 non-null float64\n",
      "Attr33    42627 non-null float64\n",
      "Attr34    42627 non-null float64\n",
      "Attr35    42627 non-null float64\n",
      "Attr36    42627 non-null float64\n",
      "Attr37    42627 non-null float64\n",
      "Attr38    42627 non-null float64\n",
      "Attr39    42627 non-null float64\n",
      "Attr40    42627 non-null float64\n",
      "Attr41    42627 non-null float64\n",
      "Attr42    42627 non-null float64\n",
      "Attr43    42627 non-null float64\n",
      "Attr44    42627 non-null float64\n",
      "Attr45    42627 non-null float64\n",
      "Attr46    42627 non-null float64\n",
      "Attr47    42627 non-null float64\n",
      "Attr48    42627 non-null float64\n",
      "Attr49    42627 non-null float64\n",
      "Attr50    42627 non-null float64\n",
      "Attr51    42627 non-null float64\n",
      "Attr52    42627 non-null float64\n",
      "Attr53    42627 non-null float64\n",
      "Attr54    42627 non-null float64\n",
      "Attr55    42627 non-null float64\n",
      "Attr56    42627 non-null float64\n",
      "Attr57    42627 non-null float64\n",
      "Attr58    42627 non-null float64\n",
      "Attr59    42627 non-null float64\n",
      "Attr60    42627 non-null float64\n",
      "Attr61    42627 non-null float64\n",
      "Attr62    42627 non-null float64\n",
      "Attr63    42627 non-null float64\n",
      "Attr64    42627 non-null float64\n",
      "class     42627 non-null int64\n",
      "dtypes: float64(64), int64(1)\n",
      "memory usage: 21.5 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if it is balanced or imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    40578\n",
       "1     2049\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### it is imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "model=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Attr1', 'Attr2', 'Attr3', 'Attr4', 'Attr5', 'Attr6', 'Attr7', 'Attr8',\n",
       "       'Attr9', 'Attr10', 'Attr11', 'Attr12', 'Attr13', 'Attr14', 'Attr15',\n",
       "       'Attr16', 'Attr17', 'Attr18', 'Attr19', 'Attr20', 'Attr21', 'Attr22',\n",
       "       'Attr23', 'Attr24', 'Attr25', 'Attr26', 'Attr27', 'Attr28', 'Attr29',\n",
       "       'Attr30', 'Attr31', 'Attr32', 'Attr33', 'Attr34', 'Attr35', 'Attr36',\n",
       "       'Attr37', 'Attr38', 'Attr39', 'Attr40', 'Attr41', 'Attr42', 'Attr43',\n",
       "       'Attr44', 'Attr45', 'Attr46', 'Attr47', 'Attr48', 'Attr49', 'Attr50',\n",
       "       'Attr51', 'Attr52', 'Attr53', 'Attr54', 'Attr55', 'Attr56', 'Attr57',\n",
       "       'Attr58', 'Attr59', 'Attr60', 'Attr61', 'Attr62', 'Attr63', 'Attr64',\n",
       "       'class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(['class'],axis=1)\n",
    "y=df['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91956\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8111,   21],\n",
       "       [ 391,    3]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9516772226131832"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.iloc[:,:-1].values\n",
    "y=df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT=DecisionTreeClassifier()\n",
    "DT.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=DT.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 95.48440065681444\n"
     ]
    }
   ],
   "source": [
    "accuracy=metrics.accuracy_score(y_test,y_pred)\n",
    "print(f\"Accuracy is: {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling imbalanced data by editednearest neighbours. it is for undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import  EditedNearestNeighbours\n",
    "enn=EditedNearestNeighbours()\n",
    "x_enn,y_enn=enn.fit_resample(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_enn_train,x_enn_test,y_enn_train,y_enn_test=train_test_split(x_enn,y_enn,test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37575, 64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_enn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37575,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_enn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91956\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_enn_train,y_enn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_enn_pred=model.predict(x_enn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9434464404524284\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_enn_test,y_enn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      7113\n",
      "           1       0.26      0.03      0.05       402\n",
      "\n",
      "    accuracy                           0.94      7515\n",
      "   macro avg       0.60      0.51      0.51      7515\n",
      "weighted avg       0.91      0.94      0.92      7515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_enn_test,y_enn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7078  390]\n",
      " [  35   12]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_enn_pred,y_enn_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "x_ada,y_ada=ADASYN().fit_resample(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ada_train,x_ada_test,y_ada_train,y_ada_test=train_test_split(x_ada,y_ada,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81835, 64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ada.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81835,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ada.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT.fit(x_ada_train,y_ada_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ada_pred=DT.predict(x_ada_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91     10129\n",
      "           1       0.90      0.93      0.91     10330\n",
      "\n",
      "    accuracy                           0.91     20459\n",
      "   macro avg       0.91      0.91      0.91     20459\n",
      "weighted avg       0.91      0.91      0.91     20459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_ada_test,y_ada_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9116770125617087\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_ada_pred,y_ada_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm=SMOTE()\n",
    "x_sm,y_sm=sm.fit_resample(x,y)\n",
    "x_sm_train,x_sm_test,y_sm_train,y_sm_test=train_test_split(x_sm,y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT.fit(x_sm_train,y_sm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sm_pred=DT.predict(x_sm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91      9930\n",
      "           1       0.93      0.90      0.92     10359\n",
      "\n",
      "    accuracy                           0.91     20289\n",
      "   macro avg       0.91      0.91      0.91     20289\n",
      "weighted avg       0.91      0.91      0.91     20289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_sm_pred,y_sm_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9144363941051802\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_sm_test,y_sm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
